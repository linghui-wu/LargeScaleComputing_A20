{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Cluster Computing in Dask with RCC and AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Start of Using Dask Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to learn about [**Dask**](https://dask.org/), a powerful and comprehensive cluster computing library in python. To install\n",
    "dask on your local machine, just run the following command:\n",
    "\n",
    "`python -m pip install \"dask[complete]\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to use the \"complete\" syntax here, which would automatically install all the core packages in dask. Besides, when running on the RCC system, where the python environment is managed by anaconda, it is highly recommended to create a virtual environment to hold dask. You can either run `conda install dask` or the command above to install dask in your conda environment.\n",
    "\n",
    "To use dask on a local single machine (e.g., your laptop), there is nothing else to install. You could simply run the following commands to start a local cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22177789ed834c898debfe38199ad5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>LocalCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()  # sceduler_port=1234\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a local cluster, the default number of processes started equals to the number of CPUs on the local machine. To adjust the number of workers (processes) in dask, you can use the scale method as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(4) #your number of processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask provides users with a convenient interface to monitor the usage and status of the workers in the cluster. To open the monitor, you just need to type `localhost:8787` after launching the cluster. 8787 is the default port number for the diagonastic server, you can also customize the port number by specifying the `dashboard_address` argument when setting up the cluster. More options to customize the local cluster configuration could be found [here](https://docs.dask.org/en/latest/setup/single-distributed.html).\n",
    "\n",
    "\n",
    "<img src=\"./dask_worker monitor.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the resources in the cluster, we need a scheduler to coordinate the operation and storage of the data, and this is why the client object in dask comes into being. The client object provides some functions similar to pool.map (map) and pool.apply (submit) in python multiprocessing, and also functions analogous to the scatter and gather methods in MPI4PY to handle large iterable objects and collect map results. Below it's a simple example of how to use these methods in dask, and all the data operation methods and data types, which we would mention in a moment, can all be used in the same way once the cluster is configured properly, no matter on a single machine or in a multi-node system (RCC) or a cloud computing platform (AWS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "def digit_sum(x):\n",
    "    string = str(x)\n",
    "    return sum([int(c) for c in string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# submit single jobs and get the result\n",
    "result_future = client.submit(digit_sum,1234)\n",
    "result_future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62, 42, 40, 34, 57, 41, 40, 29, 57, 41]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit map jobs and get the result\n",
    "results_future = client.map(digit_sum, np.random.randint(1e+8, 1e+9, int(1e+4)))\n",
    "results = client.gather(results_future)\n",
    "results[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can firstly scatter the data before mapping, this is useful when elements in the iterable are large. This generally works for python built-in iterables like list, tuple, set and dictionaries. For numpy arrays, please use client.map directly or dask.array to parallelize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <Future: finished, type: numpy.int64, key: 0>,\n",
       " 1: <Future: finished, type: numpy.int64, key: 1>,\n",
       " 2: <Future: finished, type: numpy.int64, key: 2>,\n",
       " 3: <Future: finished, type: numpy.int64, key: 3>,\n",
       " 4: <Future: finished, type: numpy.int64, key: 4>,\n",
       " 5: <Future: finished, type: numpy.int64, key: 5>,\n",
       " 6: <Future: finished, type: numpy.int64, key: 6>,\n",
       " 7: <Future: finished, type: numpy.int64, key: 7>,\n",
       " 8: <Future: finished, type: numpy.int64, key: 8>,\n",
       " 9: <Future: finished, type: numpy.int64, key: 9>,\n",
       " 10: <Future: finished, type: numpy.int64, key: 10>,\n",
       " 11: <Future: finished, type: numpy.int64, key: 11>,\n",
       " 12: <Future: finished, type: numpy.int64, key: 12>,\n",
       " 13: <Future: finished, type: numpy.int64, key: 13>,\n",
       " 14: <Future: finished, type: numpy.int64, key: 14>,\n",
       " 15: <Future: finished, type: numpy.int64, key: 15>,\n",
       " 16: <Future: finished, type: numpy.int64, key: 16>,\n",
       " 17: <Future: finished, type: numpy.int64, key: 17>,\n",
       " 18: <Future: finished, type: numpy.int64, key: 18>,\n",
       " 19: <Future: finished, type: numpy.int64, key: 19>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_future = client.scatter({x:y for x,y in enumerate(np.random.randint(1e+8,1e+9,20))})\n",
    "data_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 51, 43, 49, 41, 28, 39, 54, 42, 52]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_future = client.map(digit_sum, data_future.values())\n",
    "results = client.gather(results_future)\n",
    "results[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this example, you see that dask would not directly send back the computation result but store them in the future objects instead. Future objects are stored in the shared memory for each processes. An advantage of this mechanism is that it can avoid unnecessary communications between processes, which cost extra time. This design is suitable for build up process pipelines, where we would have one or more intermediate results to pass through the whole process. That is to say, you can pass the intermediate data to another client.map function for further computation without the effort to manually send the data from local process to the shared memory.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask also provides interface for users to check the address, status, etc. of individual workers in code by defining the Nanny object. To check this, call cluster.workers and its affliated functions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <Nanny: tcp://127.0.0.1:61531, threads: 2>,\n",
       " 1: <Nanny: tcp://127.0.0.1:61532, threads: 2>,\n",
       " 2: <Nanny: tcp://127.0.0.1:61529, threads: 2>,\n",
       " 3: <Nanny: tcp://127.0.0.1:61530, threads: 2>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To restart (this means killing all currently running jobs and cleaning the memory) run the following, the restarted worker could have a different address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Nanny.restart at 0x7ff8e50d5200>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_index = 0\n",
    "cluster.workers[worker_index].restart(timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To restart the cluster, use `client.restart()`. To shut down the local cluster, use `cluster.close()`. You do not have to manually shut down the client session since it would end as long as the python kernel is terminated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Dask in RCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following tutorial, we are going to use the same datasets in the spark session to get you familiar with using dask in a multi-node cluster computing environment, the RCC system. And we would still use the example of classifying the ratings of books to show the details of deploying dask in machine learning tasks.\n",
    "\n",
    "Ideally, we should be able to read the product review dataset from Amazon S3 using [dask.read_parquet](https://docs.dask.org/en/latest/dataframe-api.html?highlight=read_parquet#dask.dataframe.read_parquet). However, the RCC system is designed to block Internet connection on the compute nodes, which would be used as components of our Dask cluster. So we are unable to use Dask to parallelly read the external datasets from S3 on RCC. To make it work, we have manually downloaded the book review datasets to the directory of our class resource on RCC ('/project2/macs30123/'), and we can read the data \"locally\" on RCC now.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the nodes of RCC with dask, we need to install both dask and dask-jobqueue modules in our conda environment as follow. The [dask-jobqueue](https://jobqueue.dask.org/en/latest/) is used to deploy dask on common job queuing systems like PBS (used in SSD acropolis) and Slurm (used in RCC). It allows us to automatically generate sbatch scripts and directly submit them to the job manager system interactively in jupyter notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "! pip install \"dask[complete]\" dask-jobqueue --upgrade #conda install dask-jobqueue -c conda-forge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a cluster on RCC system, we use the SLURMCluster of the RCC system to define our sbatch scripts. The \"queue\" argument specifies the partition to which you would like to submit your jobs; the \"core\" argument specifies the number of cpus you would like to use for each node (not the entire number of cpus); the \"memory\" argument defines the total memory allocation for a single node in the cluster, which is evenly distributed over all cpus; the \"process\" argument integrate cores into subgroups and for most pythonic jobs it is recommmended to make each process hold one cpu to avoid the [Global Interpreter Lock](https://realpython.com/python-gil/); the 'interface' argument is used to define the type of network interface linking different nodes, which in our case is the infiniband interconnect (ib0).       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_jobqueue as jobq\n",
    "\n",
    "cluster = jobq.SLURMCluster(queue='broadwl',cores=10, memory='40GB',processes=10,\n",
    "                            walltime='01:00:00', interface='ib0', job_extra=['--account=macs30123']\n",
    "                            #job_extra=['--output=dask_worker.out',\n",
    "                            #           '--error=dask_worker.err']\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the printed information (if none, use the default port number 8787), we can open the monitor at `localhost:[port-number]`. By default, there are no workers started at this moment, you can check the generated sbatch script, which is just like our sbatch files, before finally submitting the jobs to the queue system by using the scale methods, which would create the multi-node cluster needed by our parallel analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment of the client object is an important step to connect the dask backend to the cluster you just specified. Otherwise, they would just run on a local cluster (which is forbidden by RCC). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate 2 nodes to the cluster\n",
    "cluster.scale(jobs=2)\n",
    "\n",
    "# connect the local machine to the nodes\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask was developed in coordination with other widely used python community projects like Numpy, Pandas, and Scikit-Learn. Like Spark, Dask provides users with parallelized arrays ([dask.array](https://docs.dask.org/en/latest/array.html)), dataframes ([dask.dataframe](https://docs.dask.org/en/latest/dataframe.html)) to store and process large dataset. It also includes a parallelized version of general python collection objects ([dask.bag](https://docs.dask.org/en/latest/bag.html)) for users to implement operations like map and filter. You would practice using these dask data types in courses on datacamp. For this section, we would primarily use the dask dataframes and arrays to finish the machine learning tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# load the book review data\n",
    "df = dd.read_csv('/project2/macs30123/AWS_book_reviews/*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The * symbol above is specially used in dask to read in multiple datasets in a parallel way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above data exploration, we could see that the dask dataset has many methods that are analogous to the functions in [pandas](https://pandas.pydata.org/docs/reference/index.html), so it would be very easy-to-use if you are already familiar with pandas. One major difference is the use of delay mechanism in dask dataframes, which is revealed by the compute() function. This means that dataframe operations would not be executed unless the compute method is called, and this would trigger all previously called related operations to be performed like a pipeline.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to perform the preprocessing for the book review dataset to obtain our features and labels for the machine learning task. Based on our intuition, id_like attributes are not likely to provide much useful information. Other attributes like total_votes and helpful_votes might contain more useful information about the ratings of books. We can make a mean calculation based on the groupby results to check our speculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a groupby operation\n",
    "df.groupby(by='marketplace').mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we manually encoded the categorical of vine and verified_purchase. You can also do this in dask with the [OrdinalEncoder](https://ml.dask.org/modules/generated/dask_ml.preprocessing.OrdinalEncoder.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding some coded features\n",
    "df['vine_code'] = df['vine'].apply(lambda x:1 if x=='Y' else 0, meta=('vine', 'int64'))\n",
    "df['verified_purchase_code'] = df['verified_purchase'].apply(lambda x:1 if x=='Y' else 0, meta=('vine', 'int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['vine_code','verified_purchase_code']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a groupby operation\n",
    "df.groupby(by='star_rating').mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results, it seems that our guesses are likely to be correct. For these numerical features, helpful_votes, total_votes, vine_code and verified_purchase_code would be included in our machine learning task.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All available feature engineering tools in dask are covered in [dask_ml](https://ml.dask.org/index.html), which we need to install first. To precess the text in dask, you can visit the [vectorizers](https://ml.dask.org/modules/generated/dask_ml.preprocessing.OrdinalEncoder.html) for more information. Noticing that the result of text processing are dask arrays with scipy sparse matrix internally. If you want to check the values in them, you need to use the toarray() or todense() methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install dask_ml --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with missing values \n",
    "df[['review_body','review_headline']]=df[['review_body','review_headline']].fillna(value='empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml import feature_extraction\n",
    "\n",
    "vect = feature_extraction.text.HashingVectorizer(stop_words='english',n_features=64)\n",
    "text_sparse_array = vect.fit_transform(df['review_body'])\n",
    "text_sparse_array.compute_chunk_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['star_rating'].apply(lambda x:1 if x>3 else 0, meta=('star_rating', 'int64'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('label').count().compute()['star_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick up some features we think of useful as features\n",
    "num_feature_df = df[['helpful_votes','total_votes','vine_code','verified_purchase_code']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to do some scaling here to decrease the effect of different scales for difffernt attributes. For different scale methods, you can see [here](https://ml.dask.org/modules/api.html#module-dask_ml.preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "num_feature_df = scaler.fit_transform(num_feature_df)\n",
    "num_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array\n",
    "label_array = df['label'].to_dask_array().compute_chunk_sizes()\n",
    "num_feature_array = num_feature_df.to_dask_array().compute_chunk_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth mentioning the usage of compute_chunk_sizes(), this function computes and memorizes the original size and chunk size of the datasets which is useful for deciding how many samples we need for train and test sets. Previously, the total sample size of feature and label arrays are not known because they are distributed over the cluster nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally come to the train-test section. Actually, dask hoes not developed most of their machine learning models. They developed parallel interface for a resourceful machine learning package [scikit-learn](https://scikit-learn.org/stable/). So the way we run machine learning methods in dask is quite similar to using scikit-learn.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the numerical features\n",
    "from dask_ml.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(num_feature_array, \n",
    "                                                  label_array,\n",
    "                                                  train_size=0.7,test_size=0.3,\n",
    "                                                  random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using class weights to mitigate data imbalance\n",
    "class_counts = df.groupby('label').count().compute()['star_rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask allows you to use multiple models from sklearn, and here we use the SGD classifier to perform the machine learning task. As you can see from the code, we implement cross validation and hyperparameter searching in the model, which is the best part in machine learning tasks to be parallelized. The IncrementalSearchCV we used here is designed for models that have [partial_fit](https://ml.dask.org/modules/generated/dask_ml.wrappers.Incremental.html#) functions, like the SGDClassifier. These incremental learners can train on batches of data. This fits well with Dask’s blocked data structures. Besides, the use of hyper-parameter optimizer is also related to the constraint posed by memory and the increased complexity with more parameters. For details about how to choose this the optimizer, you could view [here](https://ml.dask.org/hyper-parameter-search.html?highlight=memory#scaling-hyperparameter-searches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import IncrementalSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "#import joblib\n",
    "\n",
    "clf = SGDClassifier(class_weight={0:1,1:class_counts[0]/class_counts[1]})\n",
    "params = {'alpha': np.logspace(-4, 0, num=1000),\n",
    "          'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge'],\n",
    "          'average': [True, False]}\n",
    "#clf.fit(X_train, y_train)\n",
    "#clf = Incremental(est, scoring='accuracy')\n",
    "search = IncrementalSearchCV(clf, params)\n",
    "search.fit(X_train, y_train, classes=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the best parameter\n",
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the train accuracy\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the test accuracy\n",
    "search.best_estimator_.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "plot_confusion_matrix(search.best_estimator_, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This primitive model suffers a lot from data imbalance even after we specified weights. So you may need to find some other ways to reduce this negative effect in your assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all the features\n",
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(text_sparse_array,\n",
    "                                                  label_array,\n",
    "                                                  train_size=0.7,test_size=0.3,\n",
    "                                                  random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import dask_ml\n",
    "\n",
    "sgd = SGDClassifier(class_weight={0:1,1:class_counts[0]/class_counts[1]})\n",
    "clf = dask_ml.wrappers.Incremental(\n",
    "    sgd, scoring='accuracy',\n",
    ")\n",
    "clf.fit(X_train,y_train, classes=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_array = X_test.compute().toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "plot_confusion_matrix(clf.estimator_, X_test_array, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result here, we see that both of the two models have some drawbacks. Using the numerical (including categorical) features yield a result of high bias while using the pure text-based (review-body) features makes the overall accuracy very low. What about use both of these features? It's your turn to try! However, I have to notify you that the sparse matrix generated by the text vectors do not function well with many models, as it has some structural differences with ordinary arrays (like shape). So you might need to think about a feasible way to transfer the text vectors to normal numpy arrays and reload it to dask (This sounds somewhat inefficient, if you have better ideas, please let us know~).  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
