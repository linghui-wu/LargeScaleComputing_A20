{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jonclindaniel/LargeScaleComputing_S20/blob/master/Labs/Lab%206%20PySpark%20EDA%20and%20ML%20in%20an%20EMR%20Notebook/Local_Colab_Spark_Setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpaNU3ETh0Qc"
   },
   "source": [
    "# Setting up PySpark in a Colab Notebook\n",
    "\n",
    "You can run Spark both locally and on a cluster. Here, I'll demonstrate how you can set up Spark to run in a Colab notebook for debugging purposes. You can also set up Spark locally in this same way if you want to take advantage of multiple CPU cores on your laptop (the setup will vary slightly, though, depending on your operating system and you'll need to figure out these specifics on your own; however, this setup does also work on WSL for me).\n",
    "\n",
    "This being said, this local option should be for testing purposes on sample datasets only. If you want to run big PySpark jobs, you will want to run these in an EMR notebook (with an EMR cluster as your backend).\n",
    "\n",
    "First, we need to install Spark and PySpark, by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "R8f1D7wfaCgF",
    "outputId": "75895338-8205-406b-be0a-c674341d07d5"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "wget -q https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
    "tar -xzf spark-2.4.7-bin-hadoop2.7.tgz\n",
    "pip install pyspark findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fHjZLeId0nvR"
   },
   "source": [
    "OK, now that we have Spark, we need to set a path to it, so PySpark knows where to find it. We do this using the `os` Python library below.\n",
    "\n",
    "On my machine (WSL, Ubuntu 20.04), where I unpacked Spark in my home directory, this can be achieved with:\n",
    "```\n",
    "os.environ[\"SPARK_HOME\"] = \"~/spark-2.4.5-bin-hadoop2.7\"\n",
    "```\n",
    "\n",
    "In Colab, it is automatically downloaded to the `/content` directory, so we indicate that as its location here. Then, we run `findspark` to find Spark for us on the machine, and finally start up a SparkSession running on all available cores (`local[4]` means your code will run on 4 threads locally, `local[*]` means that your code will run as many threads as there are logical cores on your machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CljIupW0aE06"
   },
   "outputs": [],
   "source": [
    "# Set path to Spark\n",
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
    "\n",
    "# Find Spark so that we can access session within our notebook\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Start SparkSession on all available cores\n",
    "from pyspark.sql import SparkSession\n",
    "# Specify automatically the number of cores when running locally \n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VNTQOBLthDrC"
   },
   "source": [
    "Now that we've installed everything and set up our paths correctly, we can run (small) Spark jobs both in Colab notebooks and locally (for bigger jobs, you will want to run these jobs on an EMR cluster, though. Remember, for instance, that Google only allocates us one CPU core for free)!\n",
    "\n",
    "Let's make sure our setup is working by doing couple of simple things with the pyspark.sql package on the Amazon Customer Review Sample Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "fbXWBQfSAX8q",
    "outputId": "69af5811-2391-49ba-b101-311ee10adcb5"
   },
   "outputs": [],
   "source": [
    "! pip install wget\n",
    "import wget\n",
    "\n",
    "wget.download('https://s3.amazonaws.com/amazon-reviews-pds/tsv/sample_us.tsv', 'sample_data/sample_us.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KrXWEMxjeFx1"
   },
   "outputs": [],
   "source": [
    "# Read TSV file from default data download directory in Colab\n",
    "data = spark.read.csv('sample_data/sample_us.tsv',\n",
    "                      sep=\"\\t\",\n",
    "                      header=True,\n",
    "                      inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "2qvOOIYqeWw9",
    "outputId": "a43569f6-d2bb-4b67-e849-c51a4aa62758"
   },
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "ngb25JINcUNr",
    "outputId": "029777eb-e3b9-4cec-8d30-e0cf55f6fcaf"
   },
   "outputs": [],
   "source": [
    "(data.groupBy('star_rating')\n",
    "     .sum('total_votes')\n",
    "     .sort('star_rating', ascending=False)\n",
    "     .show()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Local/Colab Spark Setup",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
